{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Code Along questions\n",
    "\n",
    "For this code along we will build a spam filter!\n",
    "\n",
    "We'll use a classic dataset for this - UCI Repository SMS Spam Detection: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load and  read the dataset,  have Spark infer the data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we will import the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split,length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Spark Session\n",
    "spark = SparkSession.builder.appName('NLP').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.text('SMSSpamCollection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|ham\tGo until juro...|\n",
      "|ham\tOk lar... Jok...|\n",
      "|spam\tFree entry i...|\n",
      "|ham\tU dun say so ...|\n",
      "|ham\tNah I don't t...|\n",
      "|spam\tFreeMsg Hey ...|\n",
      "|ham\tEven my broth...|\n",
      "|ham\tAs per your r...|\n",
      "|spam\tWINNER!! As ...|\n",
      "|spam\tHad your mob...|\n",
      "|ham\tI'm gonna be ...|\n",
      "|spam\tSIX chances ...|\n",
      "|spam\tURGENT! You ...|\n",
      "|ham\tI've been sea...|\n",
      "|ham\tI HAVE A DATE...|\n",
      "|spam\tXXXMobileMov...|\n",
      "|ham\tOh k...i'm wa...|\n",
      "|ham\tEh u remember...|\n",
      "|ham\tFine if that...|\n",
      "|spam\tEngland v Ma...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As there is only one column in our dataframe, we need to seperate the features into class and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data.select(split(data.value,\"\\t\"))\\\n",
    "    .rdd.flatMap(\n",
    "            lambda x:x\n",
    "        ).toDF(schema = [\"class\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if thats th...|\n",
      "| spam|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new length feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "|  ham|I'm gonna be home...|   109|\n",
      "| spam|SIX chances to wi...|   136|\n",
      "| spam|URGENT! You have ...|   155|\n",
      "|  ham|I've been searchi...|   196|\n",
      "|  ham|I HAVE A DATE ON ...|    35|\n",
      "| spam|XXXMobileMovieClu...|   149|\n",
      "|  ham|Oh k...i'm watchi...|    26|\n",
      "|  ham|Eh u remember how...|    81|\n",
      "|  ham|Fine if thats th...|    56|\n",
      "| spam|England v Macedon...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_length = data_new.withColumn('length',length('text'))\n",
    "data_length.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print the groupy mean of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.47192873420344|\n",
      "| spam|138.6760374832664|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_length.groupBy('class').agg({'length':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you transform you raw text in to tf_idf model :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- chain the transformer Tokenizer, StopWordsRemover, CountVectorizer and IDF for text to have a final column name 'tf_idf'\n",
    "- use the transformer StringIndexer for class column into output column 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create feature with vector assembler 'tf_idf','length of as input columns into output column named 'features'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use pipeline for fit and transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: it may differ for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First we create the instances of Tokenizer, StopWordsRemover, CountVectorizer, IDF ,StringIndexer and VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol ='text',outputCol='token_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = StopWordsRemover(inputCol='token_text', outputCol='stop_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer(inputCol='stop_token', outputCol='count_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol='count_vec', outputCol='tf_idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_spam = StringIndexer(inputCol='class', outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vec = VectorAssembler(inputCols=['tf_idf', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Then we pipeline the instances and transform our dataframe(RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[ham_spam, tokenizer, stopwords, count_vector, idf, Vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorize = pipeline.fit(data_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = Vectorize.transform(data_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13465,[7,11,31,6...|\n",
      "|  0.0|(13465,[0,24,296,...|\n",
      "|  1.0|(13465,[2,13,19,3...|\n",
      "|  0.0|(13465,[0,69,80,1...|\n",
      "|  0.0|(13465,[36,134,31...|\n",
      "|  1.0|(13465,[10,67,139...|\n",
      "|  0.0|(13465,[10,53,103...|\n",
      "|  0.0|(13465,[125,184,4...|\n",
      "|  1.0|(13465,[1,46,118,...|\n",
      "|  1.0|(13465,[0,1,13,27...|\n",
      "|  0.0|(13465,[18,43,120...|\n",
      "|  1.0|(13465,[8,17,37,8...|\n",
      "|  1.0|(13465,[13,30,46,...|\n",
      "|  0.0|(13465,[39,95,217...|\n",
      "|  0.0|(13465,[552,1690,...|\n",
      "|  1.0|(13465,[30,109,11...|\n",
      "|  0.0|(13465,[82,214,37...|\n",
      "|  0.0|(13465,[0,2,49,13...|\n",
      "|  0.0|(13465,[0,74,105,...|\n",
      "|  1.0|(13465,[4,30,33,5...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect spam or Ham\n",
    "\n",
    "now use your tf-idf data to classify spam and ham\n",
    "\n",
    "feel free to use any classifier model\n",
    "\n",
    "result may differ for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I have chosen Naives-Bayes Classifier for classifying our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = clean_data.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detector = nb.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = spam_detector.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13465,[0,1,2,13,...|[-607.72724752301...|[1.0,1.4563763644...|       0.0|\n",
      "|  0.0|(13465,[0,1,2,41,...|[-1062.0221955093...|[1.0,4.1478271417...|       0.0|\n",
      "|  0.0|(13465,[0,1,4,50,...|[-840.99413177768...|[1.0,1.8842131671...|       0.0|\n",
      "|  0.0|(13465,[0,1,5,15,...|[-1004.9716464672...|[1.0,2.1621467550...|       0.0|\n",
      "|  0.0|(13465,[0,1,7,8,1...|[-1151.8602381580...|[1.0,7.2205781148...|       0.0|\n",
      "|  0.0|(13465,[0,1,9,14,...|[-538.40521520153...|[1.0,2.0804665664...|       0.0|\n",
      "|  0.0|(13465,[0,1,12,33...|[-452.28208359575...|[1.0,2.5459592334...|       0.0|\n",
      "|  0.0|(13465,[0,1,15,20...|[-670.39812910154...|[1.0,2.3655543975...|       0.0|\n",
      "|  0.0|(13465,[0,1,21,27...|[-994.15662445024...|[1.0,4.5172161287...|       0.0|\n",
      "|  0.0|(13465,[0,1,23,62...|[-1301.6231744639...|[1.0,1.1546650359...|       0.0|\n",
      "|  0.0|(13465,[0,1,27,35...|[-1490.1700272943...|[0.99999937777440...|       0.0|\n",
      "|  0.0|(13465,[0,1,71,10...|[-683.86694910012...|[1.0,2.2846069249...|       0.0|\n",
      "|  0.0|(13465,[0,2,3,4,6...|[-1276.6898823855...|[1.0,1.7860961288...|       0.0|\n",
      "|  0.0|(13465,[0,2,3,5,6...|[-2564.0865613101...|[1.0,2.4758758527...|       0.0|\n",
      "|  0.0|(13465,[0,2,3,6,9...|[-3305.1341146876...|[1.0,8.2420983431...|       0.0|\n",
      "|  0.0|(13465,[0,2,4,7,2...|[-515.16368106237...|[1.0,1.4966438071...|       0.0|\n",
      "|  0.0|(13465,[0,2,4,8,2...|[-1399.7423962739...|[1.0,3.6666390793...|       0.0|\n",
      "|  0.0|(13465,[0,2,7,27,...|[-471.28980504215...|[0.94960547090193...|       0.0|\n",
      "|  0.0|(13465,[0,2,7,43,...|[-587.76173416674...|[1.0,8.6531486973...|       0.0|\n",
      "|  0.0|(13465,[0,2,7,67,...|[-1389.5584341607...|[0.99999999995763...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the accuracy of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ham_spam =accuracy.evaluate(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive-Bayes Classifier for HAM-SPAM 0.9158380865266944\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Naive-Bayes Classifier for HAM-SPAM\",acc_ham_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
